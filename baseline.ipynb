{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n",
    "    * CRF model input format (ex. train.data):\n",
    "        ```\n",
    "        肝 O\n",
    "        功 O\n",
    "        能 O\n",
    "        6 B-med_exam\n",
    "        8 I-med_exam\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='data/train_1_update.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputFile(path):\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    for data in datas:\n",
    "        data=data.split('\\n')\n",
    "        content=data[0]\n",
    "        trainingset.append(content)\n",
    "        annotations=data[1:]\n",
    "        for annot in annotations[1:]:\n",
    "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
    "            position.extend(annot)\n",
    "            mentions[annot[3]]=annot[4]\n",
    "    \n",
    "    return trainingset, position, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRFFormatData(trainingset, position, path):\n",
    "    if (os.path.isfile(path)):\n",
    "        os.remove(path)\n",
    "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
    "\n",
    "    # output file lines\n",
    "    count = 0 # annotation counts in each content\n",
    "    tagged = list()\n",
    "    for article_id in range(len(trainingset)):\n",
    "        trainingset_split = list(trainingset[article_id])\n",
    "        while '' or ' ' in trainingset_split:\n",
    "            if '' in trainingset_split:\n",
    "                trainingset_split.remove('')\n",
    "            else:\n",
    "                trainingset_split.remove(' ')\n",
    "        start_tmp = 0\n",
    "        for position_idx in range(0,len(position),5):\n",
    "            if int(position[position_idx]) == article_id:\n",
    "                count += 1\n",
    "                if count == 1:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos == 0:\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][0:start_pos])\n",
    "                        whole_token = trainingset[article_id][0:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token[0] == '':\n",
    "                                if token_idx == 1:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "                            else:\n",
    "                                if token_idx == 0:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "\n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    start_tmp = end_pos\n",
    "                else:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos<start_tmp:\n",
    "                        continue\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
    "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                    for token_idx in range(len(token)):\n",
    "                        if len(token[token_idx].replace(' ','')) == 0:\n",
    "                            continue\n",
    "                        # BIO states\n",
    "                        if token[0] == '':\n",
    "                            if token_idx == 1:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        else:\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        \n",
    "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                        outputfile.write(output_str)\n",
    "                    start_tmp = end_pos\n",
    "\n",
    "        token = list(trainingset[article_id][start_tmp:])\n",
    "        whole_token = trainingset[article_id][start_tmp:]\n",
    "        for token_idx in range(len(token)):\n",
    "            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "            outputfile.write(output_str)\n",
    "\n",
    "        count = 0\n",
    "    \n",
    "        output_str = '\\n'\n",
    "        outputfile.write(output_str)\n",
    "        ID = trainingset[article_id]\n",
    "\n",
    "        if article_id%10 == 0:\n",
    "            print('Total complete articles:', article_id)\n",
    "\n",
    "    # close output file\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset, position, mentions=loadInputFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total complete articles: 0\n",
      "Total complete articles: 10\n",
      "Total complete articles: 20\n",
      "Total complete articles: 30\n",
      "Total complete articles: 40\n",
      "Total complete articles: 50\n",
      "Total complete articles: 60\n",
      "Total complete articles: 70\n",
      "Total complete articles: 80\n",
      "Total complete articles: 90\n",
      "Total complete articles: 100\n",
      "Total complete articles: 110\n"
     ]
    }
   ],
   "source": [
    "data_path='data/sample.data'\n",
    "CRFFormatData(trainingset, position, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER model\n",
    "### CRF (Conditional Random Field model)\n",
    "* Using `sklearn-crfsuite` API\n",
    "\n",
    "    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRF(x_train, y_train, x_test, y_test):\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(x_train, y_train)\n",
    "    # print(crf)\n",
    "    y_pred = crf.predict(x_test)\n",
    "    y_pred_mar = crf.predict_marginals(x_test)\n",
    "\n",
    "    # print(y_pred_mar)\n",
    "\n",
    "    labels = list(crf.classes_)\n",
    "    labels.remove('O')\n",
    "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
    "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
    "    print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n",
    "    return y_pred, y_pred_mar, f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Input: \n",
    "* input features:\n",
    "    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n",
    "    \n",
    "    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "# get a dict of tokens (key) and their pretrained word vectors (value)\n",
    "# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
    "dim = 0\n",
    "word_vecs= {}\n",
    "# open pretrained word vector file\n",
    "with open('cna.cbow.cwe_p.tar_g.512d.0.txt',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split()\n",
    "\n",
    "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
    "        if len(tokens) == 2:\n",
    "            dim = int(tokens[1])\n",
    "            continue\n",
    "    \n",
    "        word = tokens[0] \n",
    "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
    "        word_vecs[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size:  158566  word_vector_dim:  (512,)\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split data into training dataset and testing dataset,\n",
    "however, we'll provide `development data` and `test data` which is real testing dataset.\n",
    "\n",
    "You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load `train.data` and separate into a list of labeled data of each text\n",
    "# return:\n",
    "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
    "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
    "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
    "from sklearn.model_selection import train_test_split\n",
    "def Dataset(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n",
    "    data_list, data_list_tmp = list(), list()\n",
    "    article_id_list=list()\n",
    "    idx=0\n",
    "    for row in data:\n",
    "        data_tuple = tuple()\n",
    "        if row == '\\n':\n",
    "            article_id_list.append(idx)\n",
    "            idx+=1\n",
    "            data_list.append(data_list_tmp)\n",
    "            data_list_tmp = []\n",
    "        else:\n",
    "            row = row.strip('\\n').split(' ')\n",
    "            data_tuple = (row[0], row[1])\n",
    "            data_list_tmp.append(data_tuple)\n",
    "    if len(data_list_tmp) != 0:\n",
    "        data_list.append(data_list_tmp)\n",
    "    \n",
    "    # here we random split data into training dataset and testing dataset\n",
    "    # but you should take `development data` or `test data` as testing data\n",
    "    # At that time, you could just delete this line, \n",
    "    # and generate data_list of `train data` and data_list of `development/test data` by this function\n",
    "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
    "                                                                                                    article_id_list,\n",
    "                                                                                                    test_size=0.33,\n",
    "                                                                                                    random_state=42)\n",
    "    \n",
    "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up word vectors\n",
    "# turn each word into its pretrained word vector\n",
    "# return a list of word vectors corresponding to each token in train.data\n",
    "def Word2Vector(data_list, embedding_dict):\n",
    "    embedding_list = list()\n",
    "\n",
    "    # No Match Word (unknown word) Vector in Embedding\n",
    "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
    "\n",
    "    for idx_list in range(len(data_list)):\n",
    "        embedding_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            key = data_list[idx_list][idx_tuple][0] # token\n",
    "\n",
    "            if key in embedding_dict:\n",
    "                value = embedding_dict[key]\n",
    "            else:\n",
    "                value = unk_vector\n",
    "            embedding_list_tmp.append(value)\n",
    "        embedding_list.append(embedding_list_tmp)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input features: pretrained word vectors of each token\n",
    "# return a list of feature dicts, each feature dict corresponding to each token\n",
    "def Feature(embed_list):\n",
    "    feature_list = list()\n",
    "    for idx_list in range(len(embed_list)):\n",
    "        feature_list_tmp = list()\n",
    "        for idx_tuple in range(len(embed_list[idx_list])):\n",
    "            feature_dict = dict()\n",
    "            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n",
    "                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n",
    "            feature_list_tmp.append(feature_dict)\n",
    "        feature_list.append(feature_list_tmp)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels of each tokens in train.data\n",
    "# return a list of lists of labels\n",
    "def Preprocess(data_list):\n",
    "    label_list = list()\n",
    "    for idx_list in range(len(data_list)):\n",
    "        label_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
    "        label_list.append(label_list_tmp)\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainembed_list = Word2Vector(traindata_list, word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = list()\n",
    "\n",
    "# No Match Word (unknown word) Vector in Embedding\n",
    "unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
    "\n",
    "for idx_list in range(len(data_list)):\n",
    "    embedding_list_tmp = list()\n",
    "    for idx_tuple in range(len(data_list[idx_list])):\n",
    "        key = data_list[idx_list][idx_tuple][0] # token\n",
    "\n",
    "        if key in word_vecs:\n",
    "            value = embedding_dict[key]\n",
    "        else:\n",
    "            value = unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-time',\n",
       " 'I-time',\n",
       " 'I-time',\n",
       " 'I-time',\n",
       " 'I-time',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-time',\n",
       " 'I-time',\n",
       " 'I-time',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-time',\n",
       " 'I-time',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-time',\n",
       " 'I-time',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-time',\n",
       " 'I-time',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-time',\n",
       " 'I-time',\n",
       " 'I-time',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Word Embedding\n",
    "trainembed_list = Word2Vector(traindata_list, word_vecs)\n",
    "testembed_list = Word2Vector(testdata_list, word_vecs)\n",
    "\n",
    "# CRF - Train Data (Augmentation Data)\n",
    "x_train = Feature(trainembed_list)\n",
    "y_train = Preprocess(traindata_list)\n",
    "\n",
    "# CRF - Test Data (Golden Standard)\n",
    "x_test = Feature(testembed_list)\n",
    "y_test = Preprocess(testdata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALEX\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
      "C:\\Users\\ALEX\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=['B-ID', 'I-ID', 'B-clinical_event', 'I-clinical_event', 'B-contact', 'I-contact', 'B-education', 'I-education', 'B-family', 'I-family', 'B-location', 'I-location', 'B-med_exam', 'I-med_exam', 'B-money', 'I-money', 'B-name', 'I-name', 'B-organization', 'I-organization', 'B-profession', 'I-profession', 'B-time', 'I-time'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "C:\\Users\\ALEX\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALEX\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "            B-ID      0.000     0.000     0.000         4\n",
      "            I-ID      0.000     0.000     0.000         8\n",
      "B-clinical_event      0.000     0.000     0.000         4\n",
      "I-clinical_event      0.000     0.000     0.000        12\n",
      "       B-contact      0.000     0.000     0.000         8\n",
      "       I-contact      0.000     0.000     0.000        24\n",
      "     B-education      0.000     0.000     0.000         1\n",
      "     I-education      0.000     0.000     0.000         1\n",
      "        B-family      0.000     0.000     0.000         7\n",
      "        I-family      0.167     0.143     0.154         7\n",
      "      B-location      0.649     0.500     0.565        48\n",
      "      I-location      0.448     0.413     0.430        63\n",
      "      B-med_exam      0.778     0.084     0.152        83\n",
      "      I-med_exam      0.846     0.146     0.249       151\n",
      "         B-money      0.364     0.250     0.296        16\n",
      "         I-money      0.375     0.265     0.310        34\n",
      "          B-name      0.795     0.500     0.614        62\n",
      "          I-name      0.725     0.515     0.602        97\n",
      "  B-organization      0.000     0.000     0.000         0\n",
      "  I-organization      0.000     0.000     0.000         0\n",
      "    B-profession      0.000     0.000     0.000         5\n",
      "    I-profession      0.000     0.000     0.000        13\n",
      "          B-time      0.656     0.465     0.544       484\n",
      "          I-time      0.770     0.618     0.686      1045\n",
      "\n",
      "       micro avg      0.713     0.480     0.574      2177\n",
      "       macro avg      0.274     0.162     0.192      2177\n",
      "    weighted avg      0.695     0.480     0.550      2177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5499339993553221"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data\n",
    "* Change model output into `output.tsv` \n",
    "* Only accept this output format uploading to competition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
    "for test_id in range(len(y_pred)):\n",
    "    pos=0\n",
    "    start_pos=None\n",
    "    end_pos=None\n",
    "    entity_text=None\n",
    "    entity_type=None\n",
    "    for pred_id in range(len(y_pred[test_id])):\n",
    "        if y_pred[test_id][pred_id][0]=='B':\n",
    "            start_pos=pos\n",
    "            entity_type=y_pred[test_id][pred_id][2:]\n",
    "        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and y_pred[test_id][pred_id+1][0]=='O':\n",
    "            end_pos=pos\n",
    "            entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n",
    "            line=str(testdata_article_id_list[test_id])+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n",
    "            output+=line+'\\n'\n",
    "        pos+=1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='output.tsv'\n",
    "with open(output_path,'w',encoding='utf-8') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id\tstart_position\tend_position\tentity_text\tentity_type\n",
      "44\t423\t428\t一百六十八\tmed_exam\n",
      "44\t693\t697\t九月三號\ttime\n",
      "44\t795\t798\t四五八\ttime\n",
      "44\t922\t926\t七二點三\ttime\n",
      "44\t930\t934\t七二點三\ttime\n",
      "44\t1100\t1103\t鞏醫師\tname\n",
      "44\t1284\t1286\t小華\tname\n",
      "44\t1290\t1292\t小明\tname\n",
      "47\t4\t6\t小明\tname\n",
      "47\t152\t154\t爬樓\ttime\n",
      "47\t156\t160\t爬一層樓\ttime\n",
      "47\t173\t176\t兩層樓\tlocation\n",
      "47\t215\t219\t六個禮拜\ttime\n",
      "47\t223\t227\t四月三號\ttime\n",
      "47\t253\t257\t六個禮拜\ttime\n",
      "47\t381\t385\t下個禮拜\ttime\n",
      "47\t388\t392\t下個禮拜\ttime\n",
      "47\t663\t666\t下禮拜\ttime\n",
      "47\t686\t689\t下禮拜\ttime\n",
      "47\t1137\t1142\t六十二點三\tmed_exam\n",
      "47\t1146\t1151\t六十二點三\tmed_exam\n",
      "47\t1289\t1297\t下個禮拜四月十號\ttime\n",
      "47\t1370\t1377\t上個月，四二三\ttime\n",
      "4\t482\t484\t半年\ttime\n",
      "55\t359\t362\t林醫師\tname\n",
      "55\t606\t609\t兩個月\ttime\n",
      "55\t678\t682\t張說明書\tname\n",
      "55\t685\t689\t張說明書\tname\n",
      "55\t945\t948\t林醫師\tname\n",
      "55\t1160\t1163\t下禮拜\ttime\n",
      "26\t32\t35\t前三天\ttime\n",
      "26\t92\t95\t三七多\ttime\n",
      "26\t690\t693\t王醫師\tname\n",
      "26\t698\t701\t王醫師\tname\n",
      "26\t780\t782\t，樓\tlocation\n",
      "26\t1606\t1609\t一千多\tmoney\n",
      "26\t1640\t1643\t一千多\tmoney\n",
      "26\t1837\t1840\t九個月\ttime\n",
      "26\t2581\t2584\t一百五\tmed_exam\n",
      "64\t415\t420\t五月三十號\ttime\n",
      "64\t444\t449\t今天禮拜五\ttime\n",
      "64\t461\t465\t晚上八點\ttime\n",
      "64\t595\t597\t五月\ttime\n",
      "64\t601\t607\t五月三十一號\ttime\n",
      "64\t656\t660\t六月一號\ttime\n",
      "64\t665\t669\t六月一號\ttime\n",
      "64\t703\t710\t六月，六月一號\ttime\n",
      "64\t720\t726\t六月一號晚上\ttime\n",
      "64\t1153\t1156\t四十八\ttime\n",
      "64\t1199\t1202\t四十八\ttime\n",
      "73\t145\t147\t小美\tname\n",
      "73\t188\t191\t兩年半\ttime\n",
      "73\t196\t199\t兩年半\ttime\n",
      "73\t243\t245\t兩年\ttime\n",
      "73\t815\t819\t一個月六\ttime\n",
      "10\t205\t209\t四月一號\ttime\n",
      "10\t213\t215\t九天\ttime\n",
      "10\t270\t274\t三個禮拜\ttime\n",
      "10\t660\t664\t兩個禮拜\ttime\n",
      "10\t678\t682\t兩個禮拜\ttime\n",
      "40\t171\t173\t爬樓\ttime\n",
      "40\t176\t179\t兩層樓\tlocation\n",
      "40\t196\t199\t兩層樓\tlocation\n",
      "40\t402\t405\t下禮拜\ttime\n",
      "40\t430\t434\t0月7號\ttime\n",
      "40\t444\t447\t下禮拜\ttime\n",
      "40\t450\t452\t四周\ttime\n",
      "40\t491\t495\t四個禮拜\ttime\n",
      "40\t524\t528\t上個禮拜\ttime\n",
      "40\t536\t540\t上個禮拜\ttime\n",
      "40\t549\t553\t上個禮拜\ttime\n",
      "40\t570\t574\t下個禮拜\ttime\n",
      "40\t645\t649\t下個禮拜\ttime\n",
      "40\t653\t656\t禮拜二\ttime\n",
      "40\t1000\t1003\t下禮拜\ttime\n",
      "40\t1009\t1013\t1月2號\ttime\n",
      "40\t1015\t1017\t小櫻\tname\n",
      "40\t1273\t1276\t禮拜三\ttime\n",
      "40\t1488\t1491\t下禮拜\ttime\n",
      "40\t1497\t1500\t下禮拜\ttime\n",
      "40\t1508\t1510\t四周\ttime\n",
      "40\t1535\t1540\t禮拜二早上\ttime\n",
      "40\t1547\t1549\t小櫻\tname\n",
      "107\t138\t142\t三個禮拜\ttime\n",
      "107\t208\t212\t這個月底\ttime\n",
      "107\t322\t326\t十月二號\ttime\n",
      "107\t804\t808\t幾月幾號\ttime\n",
      "107\t821\t825\t上上個月\ttime\n",
      "107\t1028\t1032\t上上個月\ttime\n",
      "107\t1592\t1595\t下個月\ttime\n",
      "107\t1668\t1671\t兩個月\ttime\n",
      "107\t1721\t1723\t小美\tname\n",
      "107\t1964\t1966\t半年\ttime\n",
      "107\t1996\t1998\t半年\ttime\n",
      "107\t2109\t2112\t兩個月\ttime\n",
      "107\t2127\t2129\t半年\ttime\n",
      "107\t2201\t2203\t半年\ttime\n",
      "107\t2223\t2226\t三個月\ttime\n",
      "18\t828\t831\t今天騎\ttime\n",
      "18\t958\t960\t兩年\ttime\n",
      "18\t1000\t1002\t兩年\ttime\n",
      "18\t1005\t1007\t前年\ttime\n",
      "18\t1008\t1013\t四月二十七\ttime\n",
      "18\t1029\t1031\t今年\ttime\n",
      "18\t1032\t1036\t五月三號\ttime\n",
      "18\t1059\t1063\t五月三號\ttime\n",
      "18\t1072\t1076\t六個禮拜\ttime\n",
      "18\t1118\t1122\t兩個禮拜\ttime\n",
      "18\t1136\t1138\t四周\ttime\n",
      "18\t1162\t1166\t九十六塊\tmoney\n",
      "18\t1539\t1544\t八百七十二\tmed_exam\n",
      "18\t1748\t1751\t兩個月\ttime\n",
      "18\t1815\t1818\t兩個月\ttime\n",
      "18\t1822\t1825\t禮拜四\ttime\n",
      "18\t1827\t1831\t七月三號\ttime\n",
      "18\t1849\t1853\t七月二號\ttime\n",
      "18\t1921\t1923\t小美\tname\n",
      "18\t1924\t1931\t七月四號禮拜五\ttime\n",
      "18\t1942\t1946\t七月四號\ttime\n",
      "18\t1996\t1999\t第三號\ttime\n",
      "18\t2014\t2019\t前一個禮拜\ttime\n",
      "62\t76\t79\t兩個月\ttime\n",
      "62\t95\t98\t兩個月\ttime\n",
      "62\t488\t490\t姐姐\tfamily\n",
      "62\t584\t587\t前一年\ttime\n",
      "62\t880\t883\t三個月\ttime\n",
      "11\t3\t5\t小美\tname\n",
      "11\t41\t44\t十二月\ttime\n",
      "11\t74\t79\t六月十四號\ttime\n",
      "11\t246\t248\t小兒\tfamily\n",
      "11\t1176\t1180\t去年年底\ttime\n",
      "11\t1317\t1319\t半年\ttime\n",
      "11\t1737\t1739\t半年\ttime\n",
      "11\t1848\t1850\t一塊\tmoney\n",
      "11\t2158\t2160\t小美\tname\n",
      "11\t2161\t2163\t半年\ttime\n",
      "11\t2189\t2192\t前三週\ttime\n",
      "11\t2196\t2199\t前三週\ttime\n",
      "11\t2270\t2273\t幾月份\ttime\n",
      "36\t136\t139\t六月初\ttime\n",
      "36\t163\t166\t，爸爸\tfamily\n",
      "36\t172\t174\t屏東\tlocation\n",
      "36\t192\t196\t六月九號\ttime\n",
      "36\t2115\t2123\t三月初，三月二號\ttime\n",
      "36\t2127\t2131\t三月二號\ttime\n",
      "36\t2137\t2142\t三月十八號\ttime\n",
      "36\t2238\t2240\t屏東\tlocation\n",
      "36\t2480\t2485\t三月十二號\ttime\n",
      "36\t2641\t2645\t三月十八\ttime\n",
      "36\t3571\t3574\t這禮拜\ttime\n",
      "36\t4289\t4293\t五十二歲\ttime\n",
      "36\t4649\t4652\t這禮拜\ttime\n",
      "36\t4670\t4673\t下禮拜\ttime\n",
      "36\t4818\t4821\t下禮拜\ttime\n",
      "36\t4985\t4988\t三個月\ttime\n",
      "36\t5005\t5008\t三個月\ttime\n",
      "36\t5097\t5101\t三個禮拜\ttime\n",
      "36\t5162\t5165\t第三週\ttime\n",
      "89\t595\t598\t三個月\ttime\n",
      "89\t611\t614\t三個月\ttime\n",
      "89\t656\t660\t三個禮拜\ttime\n",
      "89\t849\t852\t兩個月\ttime\n",
      "89\t1522\t1524\t半年\ttime\n",
      "91\t15\t19\t前二個月\ttime\n",
      "91\t165\t169\t前兩個月\ttime\n",
      "91\t178\t182\t前兩個月\ttime\n",
      "91\t195\t198\t兩個月\ttime\n",
      "91\t260\t262\t等級\tname\n",
      "109\t432\t435\t兩個月\ttime\n",
      "109\t1053\t1056\t兩三天\ttime\n",
      "109\t1681\t1685\t第三個月\ttime\n",
      "0\t454\t457\t，山區\tlocation\n",
      "0\t809\t811\t前年\ttime\n",
      "0\t844\t846\t前年\ttime\n",
      "0\t859\t861\t前年\ttime\n",
      "0\t1048\t1052\t三月十八\ttime\n",
      "0\t2229\t2232\t兩個月\ttime\n",
      "0\t2314\t2317\t下禮拜\ttime\n",
      "0\t2326\t2329\t下禮拜\ttime\n",
      "0\t2362\t2365\t三月二\ttime\n",
      "0\t2375\t2379\t兩個禮拜\ttime\n",
      "0\t2556\t2560\t兩個禮拜\ttime\n",
      "0\t2817\t2819\t美國\tlocation\n",
      "0\t2844\t2846\t紐約\tlocation\n",
      "0\t2854\t2856\t紐約\tlocation\n",
      "0\t2870\t2873\t新社區\tlocation\n",
      "0\t2997\t3000\t下禮拜\ttime\n",
      "0\t3298\t3302\t下個禮拜\ttime\n",
      "0\t3307\t3310\t下禮拜\ttime\n",
      "0\t3335\t3337\t七號\ttime\n",
      "0\t3341\t3343\t四月\ttime\n",
      "0\t3350\t3355\t今天二十號\ttime\n",
      "0\t3361\t3366\t下禮拜七號\ttime\n",
      "0\t3372\t3377\t下禮拜九號\ttime\n",
      "0\t3383\t3387\t四月四號\ttime\n",
      "0\t3729\t3731\t五天\ttime\n",
      "0\t4191\t4196\t一百五十塊\tmoney\n",
      "88\t6\t9\t兩個月\ttime\n",
      "88\t40\t43\t兩個月\ttime\n",
      "88\t82\t86\t今天早上\ttime\n",
      "88\t814\t818\t6月3號\ttime\n",
      "88\t820\t822\t紐約\tlocation\n",
      "88\t829\t833\t6月3號\ttime\n",
      "88\t834\t836\t紐約\tlocation\n",
      "88\t892\t894\t紐約\tlocation\n",
      "88\t904\t906\t紐約\tlocation\n",
      "88\t983\t985\t周末\ttime\n",
      "104\t39\t43\t三個禮拜\ttime\n",
      "104\t97\t99\t三月\ttime\n",
      "104\t105\t107\t四月\ttime\n",
      "104\t130\t132\t四月\ttime\n",
      "104\t944\t947\t一百塊\tmoney\n",
      "104\t1121\t1123\t高雄\tlocation\n",
      "104\t1202\t1204\t台東\tlocation\n",
      "104\t1240\t1244\t柳。醫師\tname\n",
      "104\t1319\t1322\t六個月\ttime\n",
      "104\t1432\t1434\t三月\ttime\n",
      "104\t1452\t1454\t七月\ttime\n",
      "104\t1687\t1689\t半年\ttime\n",
      "104\t2184\t2189\t十年二十年\ttime\n",
      "104\t2530\t2533\t二十年\ttime\n",
      "104\t3038\t3040\t九月\ttime\n",
      "104\t3051\t3053\t九月\ttime\n",
      "104\t3074\t3077\t五個月\ttime\n",
      "104\t3079\t3084\t九月二十號\ttime\n",
      "104\t3092\t3097\t九月二十號\ttime\n",
      "104\t3100\t3103\t禮拜三\ttime\n",
      "104\t3112\t3117\t禮拜三上午\ttime\n",
      "104\t3176\t3178\t八月\ttime\n",
      "104\t3184\t3187\t八月底\ttime\n",
      "104\t3381\t3386\t禮拜六早上\ttime\n",
      "65\t7\t10\t兩個月\ttime\n",
      "65\t109\t112\t十點鐘\ttime\n",
      "65\t138\t141\t兩個月\ttime\n",
      "65\t520\t522\t小櫻\tname\n",
      "65\t823\t826\t，姊姊\tfamily\n",
      "45\t7\t10\t林小姐\tname\n",
      "45\t735\t737\t騎車\tname\n",
      "45\t2143\t2147\t下個禮拜\ttime\n",
      "45\t2160\t2164\t下個禮拜\ttime\n",
      "45\t2168\t2174\t下個禮拜幾號\ttime\n",
      "45\t2282\t2285\t下禮拜\ttime\n",
      "45\t2290\t2293\t下個月\ttime\n",
      "45\t2299\t2302\t下個月\ttime\n",
      "45\t2308\t2310\t八號\ttime\n",
      "45\t2313\t2315\t榮總\tlocation\n",
      "45\t2322\t2324\t榮總\tlocation\n",
      "45\t2340\t2344\t九月八號\ttime\n",
      "45\t2357\t2361\t九月十號\ttime\n",
      "45\t2382\t2386\t九月十號\ttime\n",
      "45\t2387\t2390\t禮拜五\ttime\n",
      "45\t2394\t2397\t禮拜五\ttime\n",
      "31\t729\t732\t林小姐\tname\n",
      "31\t1070\t1073\t兩個月\ttime\n",
      "70\t280\t283\t兩個月\ttime\n",
      "70\t293\t296\t兩個月\ttime\n",
      "70\t303\t306\t兩個月\ttime\n",
      "42\t180\t183\t五個月\ttime\n",
      "42\t190\t192\t半年\ttime\n",
      "42\t278\t281\t兩個月\ttime\n",
      "42\t329\t332\t兩個月\ttime\n",
      "42\t901\t905\t兩個禮拜\ttime\n",
      "42\t941\t944\t五個月\ttime\n",
      "42\t950\t953\t五個月\ttime\n",
      "12\t637\t639\t二月\ttime\n",
      "12\t644\t646\t二月\ttime\n",
      "12\t899\t901\t半年\ttime\n",
      "12\t923\t926\t兩三天\ttime\n",
      "12\t1262\t1264\t兩年\ttime\n",
      "12\t1272\t1274\t兩年\ttime\n",
      "12\t1691\t1693\t半年\ttime\n",
      "15\t193\t197\t兩個禮拜\ttime\n",
      "15\t202\t206\t兩個禮拜\ttime\n",
      "15\t236\t239\t六千二\tmed_exam\n",
      "15\t259\t262\t六千二\tmed_exam\n",
      "15\t489\t492\t孫子傳\tname\n",
      "15\t721\t723\t半年\ttime\n",
      "15\t741\t744\t下個月\ttime\n",
      "15\t1372\t1376\t5月5號\ttime\n",
      "15\t1380\t1384\t5月5號\ttime\n",
      "15\t1393\t1397\t5月2號\ttime\n",
      "15\t1401\t1404\t早三天\ttime\n",
      "15\t1424\t1430\t5月7號早上\ttime\n",
      "15\t1442\t1446\t5月7號\ttime\n",
      "15\t1555\t1559\t5月7號\ttime\n",
      "114\t31\t35\t第二個月\ttime\n",
      "114\t36\t40\t第三個月\ttime\n",
      "114\t58\t62\t第三個月\ttime\n",
      "114\t496\t498\t八年\ttime\n",
      "114\t972\t975\t王醫師\tname\n",
      "114\t993\t995\t七月\ttime\n",
      "114\t1024\t1027\t王醫師\tname\n",
      "114\t1057\t1059\t七月\ttime\n",
      "114\t1060\t1062\t九月\ttime\n",
      "114\t1073\t1076\t兩個月\ttime\n",
      "114\t1112\t1115\t十一月\ttime\n",
      "114\t1125\t1127\t半年\ttime\n",
      "114\t1175\t1177\t七月\ttime\n",
      "114\t1194\t1196\t一月\ttime\n",
      "114\t1220\t1222\t一月\ttime\n",
      "114\t1255\t1257\t半年\ttime\n",
      "114\t1272\t1275\t兩個月\ttime\n",
      "114\t1309\t1312\t兩個月\ttime\n",
      "114\t1315\t1318\t兩個月\ttime\n",
      "114\t1388\t1395\t兩個月，六個月\ttime\n",
      "114\t1402\t1404\t七月\ttime\n",
      "114\t2295\t2298\t下個月\ttime\n",
      "114\t2436\t2439\t十一月\ttime\n",
      "114\t2453\t2457\t1月7號\ttime\n",
      "114\t2462\t2466\t1月7號\ttime\n",
      "114\t2476\t2481\t禮拜六下午\ttime\n",
      "114\t2507\t2512\t禮拜六下午\ttime\n",
      "114\t2521\t2524\t禮拜二\ttime\n",
      "114\t2532\t2537\t禮拜二早上\ttime\n",
      "114\t2571\t2578\t1月10號早上\ttime\n",
      "114\t3118\t3124\t第一個月天天\ttime\n",
      "76\t185\t188\t兩個月\ttime\n",
      "97\t12\t16\t第二個月\ttime\n",
      "97\t65\t68\t第二天\ttime\n",
      "97\t331\t334\t三個月\ttime\n",
      "97\t1032\t1035\t三四天\ttime\n",
      "97\t1074\t1077\t三四天\ttime\n",
      "97\t1394\t1400\t一天兩天三天\ttime\n",
      "24\t86\t90\t三十多年\ttime\n",
      "24\t1060\t1063\t黃醫師\tname\n",
      "22\t81\t84\t王醫師\tname\n",
      "22\t266\t269\t兩個月\ttime\n",
      "22\t277\t280\t兩個月\ttime\n",
      "22\t286\t289\t兩個月\ttime\n",
      "22\t1417\t1421\t上禮拜四\ttime\n",
      "22\t1425\t1429\t上禮拜四\ttime\n",
      "22\t1441\t1445\t上禮拜四\ttime\n",
      "22\t1449\t1452\t禮拜五\ttime\n",
      "22\t1461\t1464\t林醫師\tname\n",
      "22\t1473\t1476\t林醫師\tname\n",
      "22\t1479\t1482\t林醫師\tname\n",
      "22\t1836\t1839\t林醫師\tname\n",
      "22\t2612\t2615\t下禮拜\ttime\n",
      "22\t2636\t2639\t下禮拜\ttime\n",
      "22\t2902\t2906\t東區新樓\tlocation\n",
      "22\t2971\t2973\t東區\tlocation\n",
      "22\t2986\t2988\t東區\tlocation\n",
      "22\t2986\t3034\t東區那邊要洗腎，當初在成大這邊要出院的時候……醫師：好沒問題。民眾：阿那個洗腎醫生跟我講區域醫院\tlocation\n",
      "22\t3091\t3096\t：區域醫院\tlocation\n",
      "22\t3217\t3221\t一個多月\ttime\n",
      "22\t3318\t3320\t麻里\tlocation\n",
      "22\t3335\t3337\t麻里\tlocation\n",
      "22\t3354\t3356\t麻里\tlocation\n",
      "22\t3368\t3370\t半年\ttime\n",
      "22\t3377\t3380\t三個月\ttime\n",
      "22\t3384\t3394\t兩個月一個月，半個月\ttime\n",
      "96\t75\t77\t兩年\ttime\n",
      "96\t492\t494\t台東\tlocation\n",
      "96\t504\t506\t台東\tlocation\n",
      "96\t511\t513\t台東\tlocation\n",
      "96\t519\t521\t台東\tlocation\n",
      "96\t533\t535\t紐約\tlocation\n",
      "96\t1931\t1935\t兩個禮拜\ttime\n",
      "96\t2733\t2736\t六月中\ttime\n",
      "96\t3036\t3038\t美國\tlocation\n",
      "96\t3042\t3044\t澳洲\tname\n",
      "56\t5\t8\t兩個月\ttime\n",
      "56\t448\t450\t明志\tlocation\n",
      "56\t1161\t1164\t兩個月\ttime\n",
      "110\t79\t81\t周末\ttime\n",
      "110\t92\t94\t周末\ttime\n",
      "110\t110\t112\t周末\ttime\n",
      "110\t1470\t1474\t第七個月\ttime\n",
      "30\t858\t860\t爾夫\tname\n",
      "30\t893\t896\t五十塊\tmoney\n",
      "30\t1161\t1164\t許醫師\tname\n",
      "30\t1406\t1410\t四十五歲\ttime\n",
      "30\t1627\t1631\t翰，明翰\tname\n",
      "30\t1635\t1640\t林明翰醫師\tname\n",
      "30\t1893\t1896\t許醫師\tname\n",
      "30\t2042\t2045\t三個月\ttime\n",
      "30\t2149\t2154\t六千還七千\tmoney\n",
      "30\t2308\t2311\t六千塊\tmoney\n",
      "30\t2819\t2822\t，武漢\tlocation\n",
      "30\t2834\t2836\t七號\ttime\n",
      "30\t2840\t2842\t七號\ttime\n",
      "30\t3044\t3047\t四十歲\ttime\n",
      "30\t3109\t3113\t四十五歲\ttime\n",
      "30\t3154\t3158\t四十五歲\ttime\n",
      "30\t3616\t3619\t七月底\ttime\n",
      "53\t5\t8\t八點五\tmed_exam\n",
      "53\t28\t32\t十八點七\ttime\n",
      "53\t376\t379\t上禮拜\ttime\n",
      "53\t507\t510\t廖醫師\tname\n",
      "53\t568\t571\t兩個月\ttime\n",
      "53\t611\t615\t兩個禮拜\ttime\n",
      "53\t637\t641\t兩個禮拜\ttime\n",
      "53\t655\t657\t四週\ttime\n",
      "53\t943\t947\t四個禮拜\ttime\n",
      "53\t982\t986\t四個禮拜\ttime\n",
      "53\t998\t1002\t十月二號\ttime\n",
      "118\t106\t108\t學校\tlocation\n",
      "118\t717\t719\t一號\ttime\n",
      "118\t851\t853\t零號\ttime\n",
      "118\t1426\t1430\t晚上八點\ttime\n",
      "118\t1462\t1465\t第三天\ttime\n",
      "118\t2778\t2780\t一號\ttime\n",
      "118\t2993\t2995\t零號\ttime\n",
      "118\t3039\t3041\t零號\ttime\n",
      "118\t3213\t3215\t零號\ttime\n",
      "118\t3515\t3518\t九十五\tmed_exam\n",
      "118\t3616\t3619\t下禮拜\ttime\n",
      "118\t3732\t3738\t下禮拜一早上\ttime\n",
      "118\t3745\t3751\t下禮拜一早上\ttime\n",
      "118\t3757\t3768\t下禮拜約禮拜二，禮拜二\ttime\n",
      "118\t3793\t3798\t禮拜三下午\ttime\n",
      "118\t3801\t3807\t過禮拜三下午\ttime\n",
      "118\t3827\t3832\t禮拜二早上\ttime\n",
      "118\t3838\t3841\t禮拜二\ttime\n",
      "118\t3849\t3855\t十月十五早上\ttime\n",
      "9\t596\t598\t一塊\tmoney\n",
      "9\t685\t687\t一塊\tmoney\n",
      "9\t1071\t1073\t大陸\tlocation\n",
      "9\t1090\t1092\t大陸\tlocation\n",
      "9\t1333\t1336\t禮拜三\ttime\n",
      "9\t1340\t1345\t下個禮拜三\ttime\n",
      "9\t1350\t1353\t禮拜三\ttime\n",
      "9\t1362\t1370\t：星期三。星期三\ttime\n",
      "9\t1434\t1437\t上午時\ttime\n",
      "9\t1510\t1513\t禮拜五\ttime\n",
      "9\t1533\t1537\t下禮拜五\ttime\n",
      "9\t1552\t1560\t下禮拜五，四月二\ttime\n",
      "9\t1570\t1573\t四月二\ttime\n",
      "9\t1578\t1583\t四月十三號\ttime\n",
      "9\t1592\t1595\t二十號\ttime\n",
      "9\t1601\t1604\t二十號\ttime\n",
      "9\t1615\t1618\t二十號\ttime\n",
      "9\t1623\t1626\t第八號\ttime\n",
      "9\t1633\t1638\t下個禮拜五\ttime\n",
      "9\t1649\t1652\t第八號\ttime\n",
      "9\t1678\t1680\t五樓\ttime\n",
      "9\t1687\t1692\t四月二十號\ttime\n",
      "9\t1701\t1703\t五樓\ttime\n",
      "9\t1708\t1711\t四月二\ttime\n",
      "9\t1718\t1721\t第八號\ttime\n",
      "9\t1867\t1869\t六月\ttime\n",
      "9\t1881\t1885\t六月一號\ttime\n",
      "9\t1998\t2002\t六月一號\ttime\n",
      "9\t2144\t2147\t兩個月\ttime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n",
    "    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n",
    "* You may try `CRF++` tool for NER tagging by CRF model\n",
    "    * [Documentation](http://taku910.github.io/crfpp/)\n",
    "    * Need design feature template\n",
    "    * Can only computed in CPU\n",
    "* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n",
    "* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n",
    "* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
