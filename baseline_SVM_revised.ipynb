{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n",
    "    * CRF model input format (ex. train.data):\n",
    "        ```\n",
    "        肝 O\n",
    "        功 O\n",
    "        能 O\n",
    "        6 B-med_exam\n",
    "        8 I-med_exam\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/Users/asus/Desktop/大四/資料探勘/final project/SampleData_deid.txt'\n",
    "file_path2='C:/Users/asus/Desktop/大四/資料探勘/final project/development_2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputFile(path):\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    for data in datas:\n",
    "        data=data.split('\\n')\n",
    "        content=data[0]\n",
    "        trainingset.append(content)\n",
    "        annotations=data[1:]\n",
    "        for annot in annotations[1:]:\n",
    "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
    "            position.extend(annot)\n",
    "            mentions[annot[3]]=annot[4]\n",
    "    \n",
    "    return trainingset, position, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRFFormatData(trainingset, position, path):\n",
    "    if (os.path.isfile(path)):\n",
    "        os.remove(path)\n",
    "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
    "\n",
    "    # output file lines\n",
    "    count = 0 # annotation counts in each content\n",
    "    tagged = list()\n",
    "    for article_id in range(len(trainingset)):\n",
    "        trainingset_split = list(trainingset[article_id])\n",
    "        while '' or ' ' in trainingset_split:\n",
    "            if '' in trainingset_split:\n",
    "                trainingset_split.remove('')\n",
    "            else:\n",
    "                trainingset_split.remove(' ')\n",
    "        start_tmp = 0\n",
    "        for position_idx in range(0,len(position),5):\n",
    "            if int(position[position_idx]) == article_id:\n",
    "                count += 1\n",
    "                if count == 1:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos == 0:\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][0:start_pos])\n",
    "                        whole_token = trainingset[article_id][0:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token[0] == '':\n",
    "                                if token_idx == 1:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "                            else:\n",
    "                                if token_idx == 0:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "\n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    start_tmp = end_pos\n",
    "                else:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos<start_tmp:\n",
    "                        continue\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
    "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                    for token_idx in range(len(token)):\n",
    "                        if len(token[token_idx].replace(' ','')) == 0:\n",
    "                            continue\n",
    "                        # BIO states\n",
    "                        if token[0] == '':\n",
    "                            if token_idx == 1:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        else:\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        \n",
    "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                        outputfile.write(output_str)\n",
    "                    start_tmp = end_pos\n",
    "\n",
    "        token = list(trainingset[article_id][start_tmp:])\n",
    "        whole_token = trainingset[article_id][start_tmp:]\n",
    "        for token_idx in range(len(token)):\n",
    "            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "            outputfile.write(output_str)\n",
    "\n",
    "        count = 0\n",
    "    \n",
    "        output_str = '\\n'\n",
    "        outputfile.write(output_str)\n",
    "        ID = trainingset[article_id]\n",
    "\n",
    "        if article_id%10 == 0:\n",
    "            print('Total complete articles:', article_id)\n",
    "\n",
    "    # close output file\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset, position, mentions=loadInputFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total complete articles: 0\n",
      "Total complete articles: 10\n",
      "Total complete articles: 20\n"
     ]
    }
   ],
   "source": [
    "data_path='C:/Users/asus/Desktop/大四/資料探勘/final project/sample.data'\n",
    "CRFFormatData(trainingset, position, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER model\n",
    "### CRF (Conditional Random Field model)\n",
    "* Using `sklearn-crfsuite` API\n",
    "\n",
    "    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Input: \n",
    "* input features:\n",
    "    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n",
    "    \n",
    "    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "# get a dict of tokens (key) and their pretrained word vectors (value)\n",
    "# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
    "dim = 0\n",
    "word_vecs= {}\n",
    "# open pretrained word vector file\n",
    "with open('cna.cbow.cwe_p.tar_g.512d.0.txt',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split()\n",
    "\n",
    "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
    "        if len(tokens) == 2:\n",
    "            dim = int(tokens[1])\n",
    "            continue\n",
    "    \n",
    "        word = tokens[0] \n",
    "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
    "        word_vecs[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size:  158566  word_vector_dim:  (512,)\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split data into training dataset and testing dataset,\n",
    "however, we'll provide `development data` and `test data` which is real testing dataset.\n",
    "\n",
    "You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load `train.data` and separate into a list of labeled data of each text\n",
    "# return:\n",
    "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
    "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
    "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
    "from sklearn.model_selection import train_test_split\n",
    "def Dataset(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n",
    "    data_list, data_list_tmp = list(), list()\n",
    "    article_id_list=list()\n",
    "    idx=0\n",
    "    for row in data:\n",
    "        data_tuple = tuple()\n",
    "        if row == '\\n':\n",
    "            article_id_list.append(idx)\n",
    "            idx+=1\n",
    "            data_list.append(data_list_tmp)\n",
    "            data_list_tmp = []\n",
    "        else:\n",
    "            row = row.strip('\\n').split(' ')\n",
    "            data_tuple = (row[0], row[1])\n",
    "            data_list_tmp.append(data_tuple)\n",
    "    if len(data_list_tmp) != 0:\n",
    "        data_list.append(data_list_tmp)\n",
    "    \n",
    "    # here we random split data into training dataset and testing dataset\n",
    "    # but you should take `development data` or `test data` as testing data\n",
    "    # At that time, you could just delete this line, \n",
    "    # and generate data_list of `train data` and data_list of `development/test data` by this function\n",
    "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
    "                                                                                                    article_id_list,\n",
    "                                                                                                    test_size=0.3,\n",
    "                                                                                                    random_state=30)\n",
    "    \n",
    "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up word vectors\n",
    "# turn each word into its pretrained word vector\n",
    "# return a list of word vectors corresponding to each token in train.data\n",
    "def Word2Vector(data_list, embedding_dict):\n",
    "    x_train=[]\n",
    "    embedding_list = list()\n",
    "\n",
    "    # No Match Word (unknown word) Vector in Embedding\n",
    "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
    "\n",
    "    for idx_list in range(len(data_list)):\n",
    "        embedding_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            key = data_list[idx_list][idx_tuple][0] # token\n",
    "\n",
    "            if key in embedding_dict:\n",
    "                value = embedding_dict[key]\n",
    "            else:\n",
    "                value = unk_vector\n",
    "            embedding_list_tmp.append(value[0:63])\n",
    "        embedding_list.append(embedding_list_tmp)\n",
    "        \n",
    "    for i in range(len(data_list)):\n",
    "        for k in range(len(data_list[i])):\n",
    "            features=embedding_list[i][k]\n",
    "            x_train.append(features)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels of each tokens in train.data\n",
    "# return a list of lists of labels\n",
    "def Preprocess(data_list):\n",
    "    y_train=[]\n",
    "    label_list = list()\n",
    "    for idx_list in range(len(data_list)):\n",
    "        label_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
    "        label_list.append(label_list_tmp)\n",
    "        \n",
    "                \n",
    "    for i in range(len(data_list)):\n",
    "        for k in range(len(data_list[i])):\n",
    "            labels=label_list[i][k]\n",
    "            y_train.append(labels)\n",
    "            \n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Embedding\n",
    "x_train = Word2Vector(traindata_list, word_vecs)\n",
    "x_test = Word2Vector(testdata_list, word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Preprocess(traindata_list)\n",
    "y_test = Preprocess(testdata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#y_pred, y_pred_mar, f1score = SVM(x_train, y_train, x_test, y_test)\n",
    "clf = svm.SVC(kernel='poly',degree=10)\n",
    "clf.fit(x_train, y_train)\n",
    "# print(crf)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# print(y_pred_mar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "    B-location      1.000     0.200     0.333        20\n",
      "    I-location      0.000     0.000     0.000        46\n",
      "    B-med_exam      0.000     0.000     0.000        23\n",
      "    I-med_exam      1.000     0.061     0.115        49\n",
      "       B-money      0.000     0.000     0.000        25\n",
      "       I-money      0.667     0.237     0.350        59\n",
      "        B-name      0.000     0.000     0.000         3\n",
      "        I-name      0.200     0.250     0.222         4\n",
      "B-organization      0.000     0.000     0.000         0\n",
      "I-organization      0.000     0.000     0.000         0\n",
      "  B-profession      0.000     0.000     0.000         1\n",
      "  I-profession      0.000     0.000     0.000         3\n",
      "        B-time      0.561     0.254     0.350       126\n",
      "        I-time      0.699     0.360     0.476       297\n",
      "\n",
      "     micro avg      0.663     0.245     0.358       656\n",
      "     macro avg      0.295     0.097     0.132       656\n",
      "  weighted avg      0.591     0.245     0.334       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = list(clf.classes_)\n",
    "labels.remove('O')\n",
    "f1_score = f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
    "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
    "print(classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3340929504266669"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_classes = len(clf.classes_)\n",
    "y_score = clf.decision_function(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data\n",
    "* Change model output into `output.tsv` \n",
    "* Only accept this output format uploading to competition system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output= \"\"\n",
    "for pred_id in range(len(y_test)):\n",
    "    output+=str(y_test[pred_id])+'\\n'\n",
    "    \n",
    "output_path='svm_output_raw_actual.tsv'\n",
    "with open(output_path,'w',encoding='utf-8') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output= \"\"\n",
    "for pred_id in range(len(y_pred)):\n",
    "    output+=str(y_pred[pred_id])+'\\n'\n",
    "    \n",
    "output_path='svm_output_raw_pred.tsv'\n",
    "with open(output_path,'w',encoding='utf-8') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "uploadtest_data_id = []  # store trainingset [content,content,...]\n",
    "uploadtest_data_text = []\n",
    "\n",
    "with open(file_path2, 'r', encoding='utf8') as f:\n",
    "    file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
    "datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "for data in datas:\n",
    "    data = data.split('\\n')\n",
    "    uploadtest_data_id.append(data[0][12:])\n",
    "    uploadtest_data_text.append(data[1])\n",
    "    \n",
    "testing_list = Word2Vector(uploadtest_data_text, word_vecs)\n",
    "# CRF - Test Data (Golden Standard)\n",
    "y_pred_upload = clf.predict(testing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= \"\"\n",
    "for pred_id in range(len(y_pred_upload)):\n",
    "    output+=str(y_pred_upload[pred_id])+'\\n'\n",
    "    \n",
    "output_path='output_SVM_1221_64D_pred.tsv'\n",
    "with open(output_path,'w',encoding='utf-8') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = []\n",
    "for i in range(len(uploadtest_data_id)):\n",
    "    for k in range(len(uploadtest_data_text[i])):\n",
    "        output_text.append(uploadtest_data_text[i][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
    "pos=0\n",
    "start_pos=None\n",
    "end_pos=None\n",
    "entity_text=None\n",
    "entity_type=None\n",
    "for pred_id in range(len(y_pred_upload)):\n",
    "    if y_pred_upload[pred_id][0]=='B':\n",
    "        start_pos=pos\n",
    "        entity_type=y_pred_upload[pred_id][2:]\n",
    "    elif start_pos is not None and y_pred_upload[pred_id][0]=='I' and y_pred_upload[pred_id+1][0]=='O':\n",
    "        end_pos=pos\n",
    "        \n",
    "        testdata_article_id = 0\n",
    "        L_testdata_list = 0\n",
    "        final_start_pos = start_pos\n",
    "        for i in range(len(uploadtest_data_id)):\n",
    "            L_testdata_list = L_testdata_list + len(uploadtest_data_text[i])\n",
    "            final_end_pos = end_pos \n",
    "            if start_pos>L_testdata_list:\n",
    "                testdata_article_id = i\n",
    "                final_start_pos = final_start_pos-L_testdata_list\n",
    "                final_end_pos = end_pos-L_testdata_list            \n",
    "        \n",
    "        entity_text=''.join([output_text[position] for position in range(start_pos,end_pos+1)])\n",
    "        \n",
    "        line=str(uploadtest_data_id[testdata_article_id])+'\\t'\n",
    "        line=line+str(final_start_pos)+'\\t'\n",
    "        line=line+str(final_end_pos+1)+'\\t'\n",
    "        line=line+entity_text+'\\t'\n",
    "        line=line+entity_type\n",
    "        output+=line+'\\n'\n",
    "    pos+=1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='output_SVM_1221_64D.tsv'\n",
    "with open(output_path,'w',encoding='utf-8') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n",
    "    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n",
    "* You may try `CRF++` tool for NER tagging by CRF model\n",
    "    * [Documentation](http://taku910.github.io/crfpp/)\n",
    "    * Need design feature template\n",
    "    * Can only computed in CPU\n",
    "* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n",
    "* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n",
    "* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
